{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"preprocessing.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"oaF4wqg-zqEz","executionInfo":{"status":"ok","timestamp":1618104361816,"user_tz":240,"elapsed":1064,"user":{"displayName":"Nicholas Lovejoy","photoUrl":"","userId":"09592189614275733627"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ee3oQpDszveZ","executionInfo":{"status":"ok","timestamp":1618104381546,"user_tz":240,"elapsed":20707,"user":{"displayName":"Nicholas Lovejoy","photoUrl":"","userId":"09592189614275733627"}},"outputId":"eb2ba526-9d94-4f6f-db0c-929ecdb7d7f8"},"source":["#mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gcxR_cAdNYtd"},"source":["# Preprocess ADMISSIONS Table"]},{"cell_type":"code","metadata":{"id":"lImwUrVVzxPU"},"source":["# Load admissions data\n","df_adm = pd.read_csv(\"/content/drive/MyDrive/w266: NLP Project/MIMIC_III_data/ADMISSIONS.csv\")\n","\n","# df_adm.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JPe9VXQFz7kw"},"source":["# Convert Strings to Dates. When converting dates, it is safer to use a datetime format. Setting the errors = 'coerce' flag allows for \n","#   missing dates but it sets it to NaT (not a datetime) when the string doesn't match the format.\n","df_adm.ADMITTIME = pd.to_datetime(df_adm.ADMITTIME, format='%Y-%m-%d %H:%M:%S', errors='coerce')\n","df_adm.DISCHTIME = pd.to_datetime(df_adm.DISCHTIME, format='%Y-%m-%d %H:%M:%S', errors='coerce')\n","df_adm.DEATHTIME = pd.to_datetime(df_adm.DEATHTIME, format='%Y-%m-%d %H:%M:%S', errors='coerce')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ldqe60g6MSs0"},"source":["Get the next unplanned admission date for each patient (if it exists). First I'll verify that the dates are in order. Then I'll use the shift() function to get the next admission date."]},{"cell_type":"code","metadata":{"id":"-BjHWW9Xz_Ic"},"source":["df_adm = df_adm.sort_values(['SUBJECT_ID', 'ADMITTIME'])\n","df_adm = df_adm.reset_index(drop=True)\n","df_adm['NEXT_ADMITTIME'] = df_adm.groupby('SUBJECT_ID').ADMITTIME.shift(-1)\n","df_adm['NEXT_ADMISSION_TYPE'] = df_adm.groupby('SUBJECT_ID').ADMISSION_TYPE.shift(-1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IfUHXpOFMfp2"},"source":["Since I want to predict unplanned re-admissions I will drop (filter out) any future admissions that are ELECTIVE so that only EMERGENCY re-admissions are measured. For rows with 'elective' admissions, replace it with NaT and NaN"]},{"cell_type":"code","metadata":{"id":"3M1MTiB1MKTs"},"source":["rows = df_adm.NEXT_ADMISSION_TYPE == 'ELECTIVE'\n","df_adm.loc[rows,'NEXT_ADMITTIME'] = pd.NaT\n","df_adm.loc[rows,'NEXT_ADMISSION_TYPE'] = np.NaN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljUhcb_nMuu8"},"source":["#It's safer to sort right before the fill incase something I did above changed the order\n","df_adm = df_adm.sort_values(['SUBJECT_ID','ADMITTIME'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_BA9M-UcM2sl"},"source":["Backfill in the values that I removed. So copy the ADMITTIME from the last emergency and paste it in the NEXT_ADMITTIME for the previous emergency. So I am effectively ignoring/skipping the ELECTIVE admission row completely. Doing this will allow me to calculate the days until the next admission."]},{"cell_type":"code","metadata":{"id":"JTVg-dR4MqGY"},"source":["# Back fill. This will take a little while.\n","df_adm[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']] = df_adm.groupby(['SUBJECT_ID'])[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']].fillna(method = 'bfill')\n","\n","# Calculate days until next admission\n","df_adm['DAYS_NEXT_ADMIT'] = (df_adm.NEXT_ADMITTIME - df_adm.DISCHTIME).dt.total_seconds()/(24*60*60)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sBbD4DF3NBBJ"},"source":["**Remove NEWBORN admissions and death events**\n","\n","According to the MIMIC site \"Newborn indicates that the HADM_ID pertains to the patient's birth.\"\n","\n","I will remove all NEWBORN admission types because in this project I'm not interested in studying births â€” my primary interest is EMERGENCY and URGENT admissions. I will remove all admissions that have a DEATHTIME because in this project I'm studying re-admissions, not mortality. And a patient who died cannot be re-admitted."]},{"cell_type":"code","metadata":{"id":"-2SZWlGgM8WV"},"source":["df_adm = df_adm.loc[df_adm.ADMISSION_TYPE != 'NEWBORN']\n","df_adm = df_adm.loc[df_adm.DEATHTIME.isnull()]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1pr3neDyNGwf"},"source":["Make Output Label\n","\n","For this problem, we are going to classify if a patient will be admitted in the next 30 days. Therefore, we need to create a variable with the output label (1 = readmitted, 0 = not readmitted)."]},{"cell_type":"code","metadata":{"id":"xZ51rOXXND9X"},"source":["df_adm['OUTPUT_LABEL'] = (df_adm.DAYS_NEXT_ADMIT < 30).astype('int')\n","df_adm['DURATION'] = (df_adm['DISCHTIME']-df_adm['ADMITTIME']).dt.total_seconds()/(24*60*60)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MJGEAWX3NfuP"},"source":["#Preprocess NOTEEVENTS Table"]},{"cell_type":"markdown","metadata":{"id":"IA2JQ4xFoeR-"},"source":["# New Section"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377},"id":"e2E2yarANTl6","executionInfo":{"status":"ok","timestamp":1617715187331,"user_tz":240,"elapsed":82129,"user":{"displayName":"Nicholas Lovejoy","photoUrl":"","userId":"09592189614275733627"}},"outputId":"d259bb83-9f5c-411f-bb24-09c112d850b9"},"source":["# Load data - The noteevents file is huge, so this will take a 1-2min if you load that .csv\n","df_notes = pd.read_csv(\"/content/drive/MyDrive/w266: NLP Project/MIMIC_III_data/NOTEEVENTS.csv\")\n","\n","df_notes.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ROW_ID</th>\n","      <th>SUBJECT_ID</th>\n","      <th>HADM_ID</th>\n","      <th>CHARTDATE</th>\n","      <th>CHARTTIME</th>\n","      <th>STORETIME</th>\n","      <th>CATEGORY</th>\n","      <th>DESCRIPTION</th>\n","      <th>CGID</th>\n","      <th>ISERROR</th>\n","      <th>TEXT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>174</td>\n","      <td>22532</td>\n","      <td>167853.0</td>\n","      <td>2151-08-04</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Discharge summary</td>\n","      <td>Report</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>175</td>\n","      <td>13702</td>\n","      <td>107527.0</td>\n","      <td>2118-06-14</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Discharge summary</td>\n","      <td>Report</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>176</td>\n","      <td>13702</td>\n","      <td>167118.0</td>\n","      <td>2119-05-25</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Discharge summary</td>\n","      <td>Report</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Admission Date:  [**2119-5-4**]              D...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>177</td>\n","      <td>13702</td>\n","      <td>196489.0</td>\n","      <td>2124-08-18</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Discharge summary</td>\n","      <td>Report</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Admission Date:  [**2124-7-21**]              ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>178</td>\n","      <td>26880</td>\n","      <td>135453.0</td>\n","      <td>2162-03-25</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Discharge summary</td>\n","      <td>Report</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Admission Date:  [**2162-3-3**]              D...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ROW_ID  ...                                               TEXT\n","0     174  ...  Admission Date:  [**2151-7-16**]       Dischar...\n","1     175  ...  Admission Date:  [**2118-6-2**]       Discharg...\n","2     176  ...  Admission Date:  [**2119-5-4**]              D...\n","3     177  ...  Admission Date:  [**2124-7-21**]              ...\n","4     178  ...  Admission Date:  [**2162-3-3**]              D...\n","\n","[5 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AAXwCYWgWEiV","executionInfo":{"status":"ok","timestamp":1617715354106,"user_tz":240,"elapsed":398,"user":{"displayName":"Nicholas Lovejoy","photoUrl":"","userId":"09592189614275733627"}},"outputId":"cda6e04a-5627-47d5-edba-a1226b67f9a5"},"source":["print(df_notes.iloc[10]['TEXT'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Admission Date:  [**2125-2-9**]              Discharge Date:   [**2125-2-16**]\n","\n","\n","Service: MEDICINE\n","\n","Allergies:\n","Zocor / Lescol\n","\n","Attending:[**Doctor Last Name 1857**]\n","Chief Complaint:\n","Chest pain\n","\n","Major Surgical or Invasive Procedure:\n","Central venous line insertion (right internal jugular vein)\n","\n","History of Present Illness:\n","Mr. [**Known lastname 1858**] is an 84 yo man with moderate aortic stenosis (outside\n","hospital echo in [**2124**] with [**Location (un) 109**] 1 cm2, gradient 28 mmHg, moderate\n","mitral regurgitation, mild aortic insufficiency), chronic left\n","ventricular systolic heart failure with EF 25-30%, hypertension,\n","hyperlipidemia, diabetes mellitus, CAD s/p CABG in [**2099**] with\n","SVG-LAD-Diagonal, SVG-OM, and SVG-RPDA-RPL, with a re-do CABG in\n","[**9-/2117**] with LIMA-LAD, SVG-OM, SVG-diagonal, and SVG-RCA. He also\n","has severe peripheral arterial disease s/p peripheral bypass\n","surgery. He presented to [**Hospital 1474**] Hospital ER this morning with\n","shortness of breath and chest pain and was found to be in heart\n","failure.\n","\n","He states he was in his usual state of health until 10:30 last\n","evening when he woke up feeling cold; 1 hour later he developed\n","moderate to severe sharp chest pain radiating across his chest\n","associated with nausea, diaphoresis, and dypsnea. The pain was\n","fairly constant and did not resolve until he was given sL NTG at\n","6 am by EMS. He has been pain free since. Presenting vitals BP\n","109/66, HR 71, O2 sat 88% on RA. CXR showed congestive heart\n","failure; initial troponin-I was mildly elevated at 0.4, CK 70.\n","He given aspirin and furosemide 80 mg IV (with ~600cc diuresis),\n","Nitropaste [**1-3**]\", and Lovenox 80 mg SQ. During the ambulance\n","transfer to the [**Hospital1 18**], he also received ~500 cc IVF for ? low\n","BP).\n","\n","On further questioning, Mr. [**Known lastname 1858**] has very poor exercise\n","tolerance due to knee pain that he attributes to osteoarthritis.\n","But he says that he gets chest pain (similar to pain he had last\n","night) with fairly minimal exertion (picking up his 11 lb cat,\n","carrying 1 gallon jug of water, first getting up from sitting to\n","walk outside or to walk to the bedroom). The pain is associated\n","with dyspnea and is relieved with few minutes rest. His symptoms\n","occur about every day to every other day and have been stable\n","over the past year. He denies orthopnea, paroxysmal nocturnal\n","dyspnea, but does endorse exertional dyspnea (he cannot identify\n","the amount of exertion required). Currently, he is dyspneic and\n","feels somewhat better sitting up; he reports no chest pain.\n","\n","ROS is also positive for a nose bleed requiring ED visit several\n","months ago (and cessation of Plavix for a few days), and\n","currently gross hematuria after Foley placement and Lovenox.\n","\n","Past Medical History:\n","1. Coronary artery disease s/p CABG twice (vide infra).\n","2. Hypertension.\n","3. Diabetes mellitus.\n","4. Hyperlipidemia.\n","5. Peripheral arterial disease with occluded left common iliac\n","artery, S/P right iliac artery stenting and femoral-to-femoral\n","bypass, further angioplasty to the right profunda.\n","5. Ischemic cardiomyopathy and chronic LV systolic heart\n","failure, reported LVEF 25-30%.\n","6. Moderate-severe aortic stenosis.\n","7. Osteoarthritis.\n","\n","CAD: Diabetes, Dyslipidemia, Hypertension\n","\n","Cardiac History: CABG in [**2099**] (SVG-LAD-Diagonal, SVG-OM, and\n","SVG-RPDA-RPL), with a re-do CABG in [**9-/2117**] (LIMA-LAD, SVG-OM,\n","SVG-diagonal, and SVG-RCA)\n","\n","Percutaneous coronary intervention, in [**2120**] anatomy as follows:\n","Patent SVG to OM1, patent SVG to PDA which filled the distal PDA\n","as well as the R-PL via a jump segment. Stump occlusion of a\n","graft presumably to the right system as well as one stump that\n","could be documented of a graft to the left. Other SVG's were not\n","able to be selectively engaged. Supravalvular aortography\n","demonstrated no other patent grafts. Patent LIMA to mid-LAD,\n","which also back-perfused the diagonal via a patent jump graft\n","that was interposed between the LAD and the diagonal.\n","\n","Social History:\n","Social history is significant for the absence of current tobacco\n","use. There is no history of alcohol abuse.\n","\n","Family History:\n","There is extensive family history of early coronary disease\n","(father died of MI at 44, one brother died in 40's, one in 50's,\n","sister had stroke).\n","\n","Physical Exam:\n","Gen: Elderly white male in NAD. Oriented x3.\n","VS T 101 BP 88/54 HR 122 in A-Fib RR 27 O2 sat 97 % on 100 %\n","NRB.\n","HEENT: NCAT. Sclera anicteric. PERRL, EOMI. Conjunctiva were\n","pink, no pallor or cyanosis of the oral mucosa. No xanthalesma.\n","Neck: JVP of near angle of the jaw.\n","CV: PMI diffuse and laterally displaced. Rate irregular, normal\n","S1, S2 with mid-late peaking 3/6 systolic murmur heart\n","throughout precordium, loudest at apex. No gallop.\n","Chest: Appear tachypneic, some accesorry muscle use. No chest\n","wall deformities, scoliosis or kyphosis. Lungs with crackles [**1-3**]\n","way up L>R.\n","Abd: Soft, NTND. No HSM or tenderness. Abd aorta not enlarged by\n","palpation. No abdominial bruits.\n","Ext: No femoral bruits, could not palpate DP or TP pulses but\n","Dopplerable.\n","Skin: No stasis dermatitis, ulcers, scars, or xanthomas.\n","\n","Pertinent Results:\n","[**2125-2-10**] 03:44AM BLOOD WBC-8.1# RBC-4.11* Hgb-13.3* Hct-37.9*\n","MCV-92 MCH-32.4* MCHC-35.0 RDW-14.1 Plt Ct-111*\n","[**2125-2-10**] 08:00PM BLOOD Neuts-74.3* Lymphs-21.9 Monos-3.0 Eos-0.7\n","Baso-0.1\n","[**2125-2-10**] 03:44AM BLOOD Plt Ct-111* LPlt-2+\n","[**2125-2-10**] 08:00PM BLOOD Fibrino-760*#\n","[**2125-2-9**] 09:15PM BLOOD Glucose-195* UreaN-30* Creat-1.4* Na-133\n","K-4.6 Cl-96 HCO3-25 AnGap-17\n","\n","CK 257* --> 189* --> 192* --> 193* --> 176 --> 82\n","[**2125-2-10**] 08:00PM BLOOD ALT-38 AST-46* AlkPhos-66 TotBili-1.0\n","DirBili-0.3 IndBili-0.7\n","\n","[**2125-2-9**] 09:15PM BLOOD CK-MB-10 MB Indx-3.9 cTropnT-0.66*\n","proBNP-[**Numeric Identifier 1859**]*\n","[**2125-2-10**] 03:44AM BLOOD CK-MB-7 cTropnT-0.69*\n","[**2125-2-10**] 11:40AM BLOOD CK-MB-8 cTropnT-0.67*\n","[**2125-2-10**] 04:55PM BLOOD CK-MB-7 cTropnT-0.65*\n","[**2125-2-10**] 08:00PM BLOOD CK-MB-7 cTropnT-0.64*\n","[**2125-2-11**] 05:41AM BLOOD CK-MB-63* MB Indx-6.3* cTropnT-2.61*\n","\n","[**2125-2-9**] 09:15PM BLOOD calTIBC-334 Ferritn-93 TRF-257\n","[**2125-2-10**] 08:00PM BLOOD TSH-5.4*\n","\n","ECG [**2125-2-9**]  9:36:38 PM\n","Rhythm is most likely sinus rhythm with frequent ventricular\n","premature beats with occasional ventricular bigeminal pattern.\n","There are also frequent atrial premature beats. Intraventricular\n","conduction defect. Left ventricular hypertrophy. ST-T wave\n","changes most likely related to left ventricular hypertrophy.\n","Compared to the previous tracing of [**2124-4-9**] ventricular premature\n","beats are more frequent, as are atrial premature beats. Clinical\n","correlation is suggested.\n","\n","CXR [**2125-2-9**]: The patient is after median sternotomy and CABG.\n","The heart size appears slightly enlarged compared to the\n","previous study.  Bilateral perihilar haziness continues toward\n","the lower lungs is new consistent with new moderate- to-severe\n","pulmonary edema.  Bilateral pleural effusion is present, also\n","new, most likely part of the heart failure.   Left and right\n","retrocardiac opacities consistent with atelectasis.\n","\n","ECHO [**2125-2-11**]: The left atrium is mildly dilated. There is mild\n","symmetric left ventricular hypertrophy with normal cavity size.\n","There is moderate regional left ventricular systolic dysfunction\n","with akinesis of the basal half of the inferior and inferolaterl\n","walls. There is mild hypokinesis of the remaining segments (LVEF\n","= 25-30 %). No masses or thrombi are seen in the left ventricle.\n","Tissue Doppler imaging suggests an increased left ventricular\n","filling pressure (PCWP>18mmHg). Right ventricular chamber size\n","is normal. with moderate global free wall hypokinesis. The\n","aortic valve leaflets are moderately thickened. There is severe\n","aortic valve stenosis (area 0.6 cm2). Trace aortic regurgitation\n","is seen. The mitral valve leaflets are mildly thickened. Mild to\n","moderate ([**1-3**]+) mitral regurgitation is seen. The pulmonary\n","artery systolic pressure could not be determined. There is no\n","pericardial effusion. Compared with the prior report (images\n","unavailable of [**2119-5-8**], left ventricular systolic function is now\n","depressed and the severity of aortic stenosis has increased.\n","\n","Brief Hospital Course:\n","Patient is a 84 yo man with CAD s/p CABG twice with daily\n","angina, presenting with chest pain, dyspnea, and congestive\n","heart failure.\n","\n","# CAD: The patient was transferred to [**Hospital1 18**] for further workup\n","and treatment of chest pain. A chest X-ray performed on\n","admission showed moderate-severe pulmonary edema. He did have a\n","stably elevated troponin thought to be related to acute heart\n","failure or demand ischemia. He was started on a Lasix drop at 10\n","mg/hr for initiation of diuresis. He initially tolerated this\n","well, however at approximately 8 pm on [**2125-2-10**], Mr. [**Known lastname 1858**] was\n","transferred from the floor to the CCU after complaining of chest\n","pain when he was sitting in bed after dinner. As he was being\n","evaluated by the housestaff, he became unresponsive and\n","developed pulseless electrical arrest. Chest compressions were\n","started, but within approximately 2 minutes, he became\n","responsive and regained a palpable pulse. His rhythm appeared to\n","be atrial fibrillation with ventricular rate initially in the\n","50s but rising to the 110's. Review of his telemetry showed that\n","he had developed atrial fibillation earlier in the evening\n","without obvious ventricular arrhythmias immediately prior to his\n","arrest (which was attributed to a vasovagal episode in the\n","setting of heart failure and aortic stenosis).  On transfer to\n","the CCU, he was started on levophed for hypotension and\n","amiodarone IV for AFib. Chest x-ray on [**2125-2-10**] showed interval\n","worsening of pulmonary edema, bilateral pleural effusions and\n","bibasilar atelectasis.  At this time his cardiac enzymes became\n","very elevated with EKG changes consistent of a NSTEMI with a CK\n","to 1006 and troponin to 3.82. On [**2125-2-11**], he had a transthoracic\n","echocardiogram which showed moderate regional left ventricular\n","systolic dysfunction with akinesis of the basal half of the\n","inferior and inferolateral walls. LVEF was 25-30 %, with severe\n","aortic stenosis and [**1-3**]+ mitral regurgitation. He was\n","aggressively diuresed with an IV Lasix drip with improvement in\n","his oxygen requirement and chest x-ray.  Levophed was\n","discontinued on [**2-12**], and blood pressures remained stable off\n","pressors with MAP's 60 - 70.  On [**2125-2-13**], he was transferred\n","back to the floor team on PO amiodarone. He remained in normal\n","sinus rhythm on telemetry on po amiodarone, and diuresis was\n","continued with a Lasix drip with good urine output and\n","improvement of renal function. He remained asymptomatic with no\n","shortness of breath or chest pain after transfer. He was\n","maintained on a heparin drip bridging to Coumadin for paroxysmal\n","atrial fibrillation. His metoprolol was held for hypotension in\n","the ICU and relative hypotension with SBP in 90s and low 100s\n","upon transfer to the floor. ACE-inhibitor was held due to\n","relative hypotension and renal insufficiency with Creat 1.7. A\n","cardiac surgery consult deemed him an acceptable candidate for a\n","3rd open heart surgery for aortic valve replacement pending\n","re-assessment of his coronary anatomy. The intermediate-term\n","plan was to allow recovery from the current episode and\n","discussion as an outpatient with his primary cardiologist\n","regarding the risks and benefits of aortic valve replacement.\n","\n","On [**2-16**], the patient became hypotensive to SBPs to\n","60s-70s after getting into a chair after breakfast. He was given\n","1L NS with no response in BP. The patient was mentating but\n","became short of breath with IVF. He had worsening EKG changes.\n","He was started on Levophed without improvement in his blood\n","pressure. He was brought to the catherization laboratory for\n","potential emergent aortic valvuloplasty and was intubated. At\n","that point, he suffered a PEA arrest and could not be\n","resuscitated. He was pronounced deceased at 12:33pm.\n","\n","# Pump: As above. The patient had severe pulmonary edema with\n","initial exam revealing crackles throughout his lung fields. He\n","was treated with a Lasix drip which was transitioned on [**2-16**] to\n","po Lasix 80 mg po twice daily.\n","\n","# Rhythm: Patient was in NSR on admission. On HD #2, he had\n","chest pain, then went into PEA arrest as described above.\n","Telemetry showed atrial fibrillation prior to the event. In the\n","CCU, he was started on amiodarone 400 mg po tid to be tapered\n","over the subsequent weeks.\n","\n","# Acute renal failure: Renal function initially declined\n","(creatinine to 2.1), then improved on Lasix gtt, but stayed 1.7\n","- 1.9 (above baseline of 1.3).\n","\n","# Hematuria: He had hematuria (no clots) after traumatic Foley\n","placement at the outside hospital. The catheter was removed on\n","[**2-16**] with gradual resolution of hematuria.\n","\n","# Diabetes: Due to acute renal failre, metformin was\n","discontinued and the patient was maintained on a Humalog sliding\n","scale with 30 units of Lantus at bedtime.\n","\n","# Hematoma. The patient developed a small hematoma at the site\n","of his right internal jugular venous access after catheter\n","removal. This was treated with local compression.\n","\n","Medications on Admission:\n","Aspirin 81 mg\n","Plavix 75 mg\n","Atenolol 50 mg\n","Isordil 5 mg [**Hospital1 **]\n","HCTZ 25 mg daily\n","Lisinopril 40 mg\n","Gemfibrozil 600 mg\n","Simvastatin 20 mg\n","Glipizide 5 mg XL daily\n","Metformin unknown dose\n","Protonix 40 mg\n","Thiamine, B12, B6, folate\n","\n","Discharge Medications:\n","None\n","\n","Discharge Disposition:\n","Expired\n","\n","Discharge Diagnosis:\n","1) Severe aortic stenosis\n","2) Coronary artery disease with non-ST segment myocardial\n","infarction\n","3) Cardiogenic shock requiring pressor support\n","4) Atrial fibrillation\n","5) Pulseless electrical activity arrest, twice\n","6) Severe acute on chronic left ventricular systolic and\n","diastolic heart failure\n","7) Acute on chronic renal failure\n","8) Traumatic hematuria\n","9) Diabetes mellitus\n","10) Hypertension\n","11) Peripheral arterial disease\n","12) Hyperlipidemia\n","\n","Discharge Condition:\n","Deceased\n","\n","Discharge Instructions:\n","None\n","\n","Followup Instructions:\n","None\n","\n","                             [**Doctor First Name **] [**First Name8 (NamePattern2) **] [**Name6 (MD) **] [**Name8 (MD) **] MD, MSC 12-339\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SsCiC5tpNj4v"},"source":["# Sort by subject_ID, HAD_ID then CHARTDATE\n","df_notes = df_notes.sort_values(by=['SUBJECT_ID','HADM_ID','CHARTDATE'])\n","# Merge notes table to admissions table\n","df_adm_notes = pd.merge(df_adm[['SUBJECT_ID','HADM_ID','ADMITTIME','DISCHTIME','DAYS_NEXT_ADMIT','NEXT_ADMITTIME','ADMISSION_TYPE','DEATHTIME','OUTPUT_LABEL','DURATION']],\n","                        df_notes[['SUBJECT_ID','HADM_ID','CHARTDATE','TEXT','CATEGORY', 'CHARTTIME']],\n","                        on = ['SUBJECT_ID','HADM_ID'],\n","                        how = 'left')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7oqCqo7Nm4q"},"source":["# Grab date only, not the time\n","df_adm_notes['ADMITTIME_C'] = df_adm_notes.ADMITTIME.apply(lambda x: str(x).split(' ')[0])\n","df_adm_notes['ADMITTIME_C'] = pd.to_datetime(df_adm_notes.ADMITTIME_C, format = '%Y-%m-%d', errors = 'coerce')\n","\n","#make sure that chartdate is in appropriate format\n","df_adm_notes['CHARTDATE'] = pd.to_datetime(df_adm_notes.CHARTDATE, format = '%Y-%m-%d', errors = 'coerce')\n","\n","# make sure that charttime is in appropriate format\n","df_adm_notes['CHARTTIME'] = pd.to_datetime(df_adm_notes.CHARTTIME, errors = 'coerce')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-i8n94ZIOZ6A"},"source":["# Gather Discharge Summaries Only\n","df_discharge = df_adm_notes[df_adm_notes['CATEGORY'] == 'Discharge summary']\n","# multiple discharge summary for one admission -> after examination -> replicated summary -> replace with the last one\n","df_discharge = (df_discharge.groupby(['SUBJECT_ID','HADM_ID']).nth(-1)).reset_index()\n","df_discharge=df_discharge[df_discharge['TEXT'].notnull()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"id2omukbQK7m"},"source":["#Handle missing data in Echo, ECG and Discharge note categories\n","def fill_missing_charttime(row):\n","  '''this function fills in chart time for the categories missing data (echo, ECG and discharge)\n","  '''\n","  if pd.isna(row['CHARTTIME']):\n","    return max(row['CHARTDATE'], row['ADMITTIME'])\n","  else:\n","    return row['CHARTTIME']\n","\n","df_adm_notes['filled_charttime'] = df_adm_notes.apply(lambda x: fill_missing_charttime(x), axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d_qjsqxbOcao"},"source":["def less_n_days_data(df_adm_notes, n):\n","  '''Function pulls all notes for a given admission that were entered up to n DAYS after the intitial day of admission.\n","  It then concatenates all notes for a given admission into one long TEXT feature.\n","\n","  Note: this groups notes by calendar day and does not represent a specific timeframe in hours\n","  '''\n","  df_less_n = df_adm_notes[\n","      ((df_adm_notes['CHARTDATE'] - df_adm_notes['ADMITTIME_C']).dt.total_seconds() / (24 * 60 * 60)) < n]\n","  df_less_n = df_less_n[df_less_n['TEXT'].notnull()]\n","  # concatenate notes into one\n","  df_concat = pd.DataFrame(df_less_n.groupby('HADM_ID')['TEXT'].apply(lambda x: \"%s\" % ' '.join(x))).reset_index()\n","  df_concat['OUTPUT_LABEL'] = df_concat['HADM_ID'].apply(\n","      lambda x: df_less_n[df_less_n['HADM_ID'] == x].OUTPUT_LABEL.values[0])\n","  \n","  return df_concat\n","\n","def less_n_hours_data(df_adm_notes, n):\n","  '''Function pulls all notes for a given admission that were entered up to n HOURS after the intitial day of admission.\n","  It then concatenates all notes for a given admission into one long TEXT feature.\n","  \n","  Note: this groups notes by calendar hour - this is a departure from the initial work published by Huang and Imasogie\n","  '''\n","  df_less_n = df_adm_notes[\n","      ((df_adm_notes['filled_charttime'] - df_adm_notes['ADMITTIME']).dt.total_seconds() / (60 * 60)) < n]\n","  df_less_n = df_less_n[df_less_n['TEXT'].notnull()]\n","  # concatenate notes into one\n","  df_concat = pd.DataFrame(df_less_n.groupby('HADM_ID')['TEXT'].apply(lambda x: \"%s\" % ' '.join(x))).reset_index()\n","  df_concat['OUTPUT_LABEL'] = df_concat['HADM_ID'].apply(\n","      lambda x: df_less_n[df_less_n['HADM_ID'] == x].OUTPUT_LABEL.values[0])\n","  \n","  return df_concat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wS4dS5_FQa7z"},"source":["#this took about 10 min to run\n","\n","##subset data by day cutoffs - this is what's done in the original paper.\n","# df_less_2_days = less_n_days_data(df_adm_notes, 2)\n","# df_less_3_days = less_n_days_data(df_adm_notes, 3)\n","\n","##subset data by hour cutoffs - this is a new iteration I created for comparison\n","df_less_24_hours = less_n_hours_data(df_adm_notes, 24)\n","# df_less_48_hours = less_n_hours_data(df_adm_notes, 48)\n","# df_less_72_hours = less_n_hours_data(df_adm_notes, 72)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SO8WpRFDRZ_a"},"source":["import re\n","\n","def preprocess1(x):\n","    y = re.sub('\\\\[(.*?)\\\\]', '', x)  # remove de-identified brackets\n","    y = re.sub('[0-9]+\\.', '', y)  # remove 1.2. since the segmenter segments based on this\n","    y = re.sub('dr\\.', 'doctor', y)\n","    y = re.sub('m\\.d\\.', 'md', y)\n","    y = re.sub('admission date:', '', y)\n","    y = re.sub('discharge date:', '', y)\n","    y = re.sub('--|__|==', '', y)\n","    return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rz0dFkNKRckU"},"source":["from tqdm import tqdm, trange"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B3uyaJNkQ6qK"},"source":["def preprocessing(df_less_n):\n","    '''\n","    step 1: clean text, remove new lines and returns strip and lower caps\n","    step 2: apply regex function preprocess1\n","    step 3: break up long text blocks into 318 word chunks - NOTE this is something we can tune later if we want to\n","    '''\n","    df_less_n['TEXT'] = df_less_n['TEXT'].fillna(' ')\n","    df_less_n['TEXT'] = df_less_n['TEXT'].str.replace('\\n', ' ')\n","    df_less_n['TEXT'] = df_less_n['TEXT'].str.replace('\\r', ' ')\n","    df_less_n['TEXT'] = df_less_n['TEXT'].apply(str.strip)\n","    df_less_n['TEXT'] = df_less_n['TEXT'].str.lower()\n","\n","    df_less_n['TEXT'] = df_less_n['TEXT'].apply(lambda x: preprocess1(x))\n","\n","    # to get 318 words chunks for readmission tasks\n","    df_len = len(df_less_n)\n","    want = pd.DataFrame({'ID': [], 'TEXT': [], 'Label': []})\n","    for i in tqdm(range(df_len)):\n","        x = df_less_n.TEXT.iloc[i].split()\n","        n = int(len(x) / 318)\n","        for j in range(n):\n","            want = want.append({'TEXT': ' '.join(x[j * 318:(j + 1) * 318]), 'Label': df_less_n.OUTPUT_LABEL.iloc[i],\n","                                'ID': df_less_n.HADM_ID.iloc[i]}, ignore_index=True)\n","        if len(x) % 318 > 10:\n","            want = want.append({'TEXT': ' '.join(x[-(len(x) % 318):]), 'Label': df_less_n.OUTPUT_LABEL.iloc[i],\n","                                'ID': df_less_n.HADM_ID.iloc[i]}, ignore_index=True)\n","\n","    return want"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rr3J4SquUqUf"},"source":["The preprocessing below for the Discharge, 2-Day and 3-Day lookback windows took a while.  Times below:\n","\n","*   1-day (by hour): 50 min\n","*   2-day (by hour): 1hr 20 min\n","*   3-day (by hour): 2hr 20 min\n","\n","\n","Note: Times seem to be reduced when using an accelerator - consuder activating the GPU before running this cell \n","\n","Uncomment the lines below (I've commented it out since I've already run preprocessing and pickled the files)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WrjSu5pwSdJx","executionInfo":{"elapsed":542672,"status":"ok","timestamp":1614719666197,"user":{"displayName":"Nicholas Lovejoy","photoUrl":"","userId":"09592189614275733627"},"user_tz":300},"outputId":"f3cca363-aec2-4c2c-cc05-fa318bb2ebec"},"source":["#uncomment individual lines below to run the preprocessing on each cutoff dataframe\n","df_less_1_by_hour = preprocessing(df_less_24_hours)\n","# df_less_2_by_hour = preprocessing(df_less_48_hours)\n","# df_less_3_by_hour = preprocessing(df_less_72_hours)\n","\n","# df_less_2 = preprocessing(df_less_2)\n","# df_less_3 = preprocessing(df_less_3)\n","\n","# df_discharge = preprocessing(df_discharge)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43921/43921 [45:29<00:00, 16.09it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xP-NCGgZGhbX","executionInfo":{"elapsed":399,"status":"ok","timestamp":1614719780756,"user":{"displayName":"Nicholas Lovejoy","photoUrl":"","userId":"09592189614275733627"},"user_tz":300},"outputId":"f888a3be-86a6-4c2c-c368-d20417b6bd0b"},"source":["df_less_1_by_hour.head()\n","len(df_less_1_by_hour)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["238835"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"UepIA1-MuRHS"},"source":["Run the code below to save preprocessed data out to pickle files"]},{"cell_type":"code","metadata":{"id":"Z2oXtFcrTZoQ"},"source":["# df_less_1_by_hour.to_pickle(\"drive/MyDrive/266: NLP/w266: NLP Project/pickle_files/df_less_1_by_hour.pkl\")\n","# df_less_2_by_hour.to_pickle(\"drive/MyDrive/266: NLP/w266: NLP Project/pickle_files/df_less_2_by_hour.pkl\")\n","# df_less_3_by_hour.to_pickle(\"drive/MyDrive/266: NLP/w266: NLP Project/pickle_files/df_less_3_by_hour.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5m7DsQHj7Il"},"source":["Run code below to read pickle files back in"]},{"cell_type":"code","metadata":{"id":"-mabc_Q-4NuI"},"source":["# df_discharge = pd.read_pickle('drive/MyDrive/266: NLP/w266: NLP Project/pickle_files/df_discharge.pkl')\n","\n","# df_less_2 = pd.read_pickle('drive/MyDrive/266: NLP/w266: NLP Project/pickle_files/df_less_2.pkl')\n","# df_less_3 = pd.read_pickle('drive/MyDrive/266: NLP/w266: NLP Project/pickle_files/df_less_3.pkl')\n","\n","# df_less_1_by_hour = pd.read_pickle('drive/MyDrive/266: NLP/w266: NLP Project/pickle_files/df_less_1_by_hour.pkl')\n","# df_less_2_by_hour = pd.read_pickle('drive/MyDrive/266: NLP/w266: NLP Project/pickle_files/df_less_2_by_hour.pkl')\n","df_less_3_by_hour = pd.read_pickle('/content/drive/MyDrive/w266: NLP Project/pickle_files/df_less_3_by_hour.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hwnUHzLkkMJd","executionInfo":{"status":"ok","timestamp":1618072199233,"user_tz":240,"elapsed":5592,"user":{"displayName":"Nick Lovejoy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaCXa6zsP5AJT7Zd5qho-5D-JFnc3PsQU-jGQ-g=s64","userId":"12395593146151220621"}},"outputId":"339476d6-5bcc-4998-c5f7-2cfc3caf059c"},"source":["#check picke file shapes\n","# print('dischage df: {}'.format(df_discharge.shape))\n","# print('2-day df: {}'.format(df_less_2.shape))\n","# print('3-day df: {}'.format(df_less_3.shape))\n","# print('1-day df (parsed by hour): {}'.format(df_less_1_by_hour.shape))\n","# print('2-day df (parsed by hour): {}'.format(df_less_2_by_hour.shape))\n","print('3-day df (parsed by hour): {}'.format(df_less_3_by_hour.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3-day df (parsed by hour): (454798, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rlHT7Fb5iQIx"},"source":["# Train-Test split\n","An example to get the train/test/split with random state:\n","note that we divide on patient admission level and share among experiments, instead of notes level. \n","This way, since our methods run on the same set of admissions, we can see the progression of readmission scores. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXyb7fBlns0Z","executionInfo":{"status":"ok","timestamp":1615343859869,"user_tz":300,"elapsed":502,"user":{"displayName":"Nick Lovejoy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaCXa6zsP5AJT7Zd5qho-5D-JFnc3PsQU-jGQ-g=s64","userId":"12395593146151220621"}},"outputId":"7bf6fbb6-7c9b-4cb1-98af-709abce7fcf2"},"source":["#pull list of admissions ID where the patient re-admitted/ did not readmit\n","readmit_ID = df_adm[df_adm.OUTPUT_LABEL == 1].HADM_ID\n","not_readmit_ID = df_adm[df_adm.OUTPUT_LABEL == 0].HADM_ID\n","\n","# subsampling to get the balanced pos/neg numbers of patients for each dataset\n","not_readmit_ID_use = not_readmit_ID.sample(n=len(readmit_ID), random_state=1)\n","\n","#list of IDs for the validation and test sets - currenlty 20% in this group\n","id_val_test_t = readmit_ID.sample(frac=0.2, random_state=1)\n","id_val_test_f = not_readmit_ID_use.sample(frac=0.2, random_state=1)\n","\n","#list of IDs for training set - excluding all IDs in validation set - currently 80% of total data\n","id_train_t = readmit_ID.drop(id_val_test_t.index)\n","id_train_f = not_readmit_ID_use.drop(id_val_test_f.index)\n","\n","#subset the va/test group created above - half go to final val set and half to final test set\n","id_val_t = id_val_test_t.sample(frac=0.5, random_state=1)\n","id_val_f = id_val_test_f.sample(frac=0.5, random_state=1)\n","id_test_t = id_val_test_t.drop(id_val_t.index)\n","id_test_f = id_val_test_f.drop(id_val_f.index)\n","\n","# test if there is overlap between train and test, should return \"array([], dtype=int64)\"\n","print('Overlapping IDs: {}'.format((pd.Index(id_test_t).intersection(pd.Index(id_train_t))).values))\n","\n","#concat the test IDs and add appropriate flag\n","id_test = pd.concat([id_test_t, id_test_f])\n","test_id_label = pd.DataFrame(data=list(zip(id_test, [1] * len(id_test_t) + [0] * len(id_test_f))),\n","                             columns=['id', 'label'])\n","\n","#concat the validation IDs and add appropriate flag\n","id_val = pd.concat([id_val_t, id_val_f])\n","val_id_label = pd.DataFrame(data=list(zip(id_val, [1] * len(id_val_t) + [0] * len(id_val_f))), columns=['id', 'label'])\n","\n","#concat the training IDs and add appropriate flag\n","id_train = pd.concat([id_train_t, id_train_f])\n","train_id_label = pd.DataFrame(data=list(zip(id_train, [1] * len(id_train_t) + [0] * len(id_train_f))),\n","                              columns=['id', 'label'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overlapping IDs: []\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7JZfye7LlIlE"},"source":["# Save data for 24 hours after admission"]},{"cell_type":"markdown","metadata":{"id":"f0Zl2Pfgvf84"},"source":["### Output final .csv files to drive\n","\n","**Note:** there is some concern about the length of notes for each admission.  There seems to be longer notes for more complex admissions, which inherantly have a higher risk of readmission.  The original authors try to mitigate this issue by adding some negative examples to the training data. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hp8AJJmTRK5G","executionInfo":{"status":"ok","timestamp":1615344028497,"user_tz":300,"elapsed":4553,"user":{"displayName":"Nick Lovejoy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaCXa6zsP5AJT7Zd5qho-5D-JFnc3PsQU-jGQ-g=s64","userId":"12395593146151220621"}},"outputId":"e18464fd-760b-41a5-d9bc-126fad597031"},"source":["#subset the data to only include appropriate examples for each train/val/test subset\n","early_train = df_less_3_by_hour[df_less_3_by_hour.ID.isin(train_id_label.id)]\n","early_val = df_less_3_by_hour[df_less_3_by_hour.ID.isin(val_id_label.id)]\n","\n","# we want to test on admissions that are not discharged already. So for less than 3 days of notes experiment,\n","# we filter out admissions discharged within 3 days\n","actionable_ID_1days = df_adm[df_adm['DURATION'] >= 1].HADM_ID\n","test_actionable_id_label = test_id_label[test_id_label.id.isin(actionable_ID_1days)]\n","early_test = df_less_3_by_hour[df_less_3_by_hour.ID.isin(test_actionable_id_label.id)]\n","\n","# early_test = df_less_1_by_hour[df_less_1_by_hour.ID.isin(test_id_label.id)]\n","\n","#run the cell below to ensure a reasonably balanced dataset\n","print('Training: {:0.2%} positive examples | {:,} total examples'.format(early_train['Label'].mean(), \n","                                                                       early_train.shape[0]))\n","print('Validation: {:0.2%} positive examples | {:,} total examples'.format(early_val['Label'].mean(), \n","                                                                       early_val.shape[0]))\n","print('Test: {:0.2%} positive examples | {:,} total examples'.format(early_test['Label'].mean(), \n","                                                                       early_test.shape[0]))\n","early_train.to_csv('/content/drive/MyDrive/w266: NLP Project/data/3days_by_hour/train.csv', index=False)\n","early_val.to_csv('/content/drive/MyDrive/w266: NLP Project/data/3days_by_hour/val.csv', index=False)\n","early_test.to_csv('/content/drive/MyDrive/w266: NLP Project/data/3days_by_hour/test.csv', index=False)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training: 55.06% positive examples | 52,363 total examples\n","Validation: 52.30% positive examples | 6,981 total examples\n","Test: 53.47% positive examples | 7,256 total examples\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qL0QyVwin9X6"},"source":["## Manually create unbalanced test set"]},{"cell_type":"code","metadata":{"id":"Mrl2Pavua4pq"},"source":["df_original_train = pd.read_csv('/content/drive/MyDrive/w266: NLP Project/data/3days_by_hour/train.csv')\n","original_train_ids = set(df_original_train['ID'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9HWYChLjoG9V","executionInfo":{"status":"ok","timestamp":1618075086482,"user_tz":240,"elapsed":4711,"user":{"displayName":"Nick Lovejoy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaCXa6zsP5AJT7Zd5qho-5D-JFnc3PsQU-jGQ-g=s64","userId":"12395593146151220621"}},"outputId":"3f7e3432-c0fb-4502-9c0b-b7d5ba59b801"},"source":["#pull list of admissions ID where the patient re-admitted/ did not readmit\n","readmit = df_adm[df_adm.OUTPUT_LABEL == 1]\n","not_readmit = df_adm[df_adm.OUTPUT_LABEL == 0]\n","\n","# subsampling to get the balanced pos/neg numbers of patients for each dataset\n","readmit_ID_use = readmit[~readmit['HADM_ID'].isin(original_train_ids)]['HADM_ID']\n","not_readmit_ID_use = not_readmit[~not_readmit['HADM_ID'].isin(original_train_ids)]['HADM_ID']\n","\n","#we have 618 positive examples, to ensure we retain the natural 6.5% + examples we see in the unbalanced\n","# data, we need a total of 9507 examples with 8889 negative exampless\n","unbalanced_not_readmit_ids = not_readmit_ID_use.sample(8889, random_state=42)\n","unbalanced_readmit_ids = readmit_ID_use\n","\n","unbalanced_test_ids = unbalanced_not_readmit_ids.append(unbalanced_readmit_ids)\n","print('Test set size: {}'.format(unbalanced_test_ids.shape[0]))\n","print('Positive examples in test set: {}'.format(unbalanced_readmit_ids.shape[0]))\n","\n","# #use IDs to create df of pre-processed & chunked training data\n","unbalanced_test_df = df_less_3_by_hour[df_less_3_by_hour['ID'].isin(unbalanced_test_ids)]\n","\n","#check label balance in the final sequence breakout\n","# unbalanced_test_df.groupby('Label').agg({'Label':'count'})\n","\n","#save df\n","unbalanced_test_df.to_csv(\n","    '/content/drive/MyDrive/w266: NLP Project/data/3days_by_hour/unbalanced_test.csv', index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test set size: 9507\n","Positive examples in test set: 618\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TI5KAZ61pLuI","executionInfo":{"status":"ok","timestamp":1618104434134,"user_tz":240,"elapsed":5620,"user":{"displayName":"Nicholas Lovejoy","photoUrl":"","userId":"09592189614275733627"}},"outputId":"6496c851-2020-45d0-f560-e3eb0da89609"},"source":["df = pd.read_csv('/content/drive/MyDrive/w266: NLP Project/data/3days_by_hour/unbalanced_test.csv')\n","\n","print('Test: {:0.2%} positive examples | {:,} total examples'.format(df['Label'].mean(), \n","                                                                       df.shape[0]))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Test: 7.91% positive examples | 95,542 total examples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n8YmJKQLtKuY","executionInfo":{"status":"ok","timestamp":1618074664242,"user_tz":240,"elapsed":462,"user":{"displayName":"Nick Lovejoy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaCXa6zsP5AJT7Zd5qho-5D-JFnc3PsQU-jGQ-g=s64","userId":"12395593146151220621"}},"outputId":"33cc7956-f556-4028-d513-160eeeddaf0d"},"source":["# df_original_train[df_original_train['ID'] == 167791]\n","# df_original_train.head()\n","len(unbalanced_not_readmit_ids)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9507"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"id":"n4aYGmtDv1zO"},"source":[""],"execution_count":null,"outputs":[]}]}